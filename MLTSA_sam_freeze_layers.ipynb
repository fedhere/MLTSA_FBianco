{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fedhere/MLTSA_FBianco/blob/main/MLTSA_sam_freeze_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGTOtqqntdEs",
        "outputId": "eb20dd0d-98bc-49df-af3b-607328fc55b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MNDOrXbpcTq"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/facebookresearch/segment-anything.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/Shareddrives/FASTlab/lightEchoes/SAMsclub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOlZMh6vtcl8",
        "outputId": "c2044303-5b8f-4fe5-fa35-7061049cd2e1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/FASTlab/lightEchoes/SAMsclub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import Resize\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "JDF_KvwNtlAO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "_n3TyZ0l2Vph"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate masks"
      ],
      "metadata": {
        "id": "LtTEu3mg0P34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sam = sam_model_registry[\"vit_h\"](checkpoint=\"sam_vit_h_4b8939.pth\")\n",
        "\n",
        "sam.to(device=device)\n",
        "\n",
        "mask_generator = SamAutomaticMaskGenerator(sam)"
      ],
      "metadata": {
        "id": "LqmlKsI-vt7v"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "Acqw-1WKy6jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "if type(uploaded) is not dict: uploaded = uploaded.files  ## Deal with filedit versions\n",
        "file_contents = uploaded[list(uploaded.keys())[0]]\n",
        "file_contents"
      ],
      "metadata": {
        "id": "mCXA9Z0zzAs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: read in the image uploaded into files content\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "image = Image.open(io.BytesIO(file_contents))\n",
        "\n",
        "image = np.array(image)\n"
      ],
      "metadata": {
        "id": "AH-j5_85zDIY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "id": "56PDoc8yzrRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K_v-5C762S3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks = mask_generator.generate(image)"
      ],
      "metadata": {
        "id": "qpMiaPhnvwv4"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks[2]"
      ],
      "metadata": {
        "id": "y6BMt1PC0-D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masks[0]['segmentation'].shape"
      ],
      "metadata": {
        "id": "xpHQ-zGi4ym7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(masks[0]['segmentation'])"
      ],
      "metadata": {
        "id": "Sq6ChQaB24NX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sam.mask_decoder.transformer.layers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2s8pYmUCYM_",
        "outputId": "a67496b4-b5f9-495a-fbfb-cc45c7dd47ab"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0-1): 2 x TwoWayAttentionBlock(\n",
              "    (self_attn): Attention(\n",
              "      (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "    )\n",
              "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (cross_attn_token_to_image): Attention(\n",
              "      (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "    )\n",
              "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (mlp): MLPBlock(\n",
              "      (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "      (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "      (act): ReLU()\n",
              "    )\n",
              "    (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    (cross_attn_image_to_token): Attention(\n",
              "      (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-RaNPTOmv6I",
        "outputId": "3dbdba57-4f0f-4cf1-de28-e4daee98cf23"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sam(\n",
              "  (image_encoder): ImageEncoderViT(\n",
              "    (patch_embed): PatchEmbed(\n",
              "      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
              "    )\n",
              "    (blocks): ModuleList(\n",
              "      (0-31): 32 x Block(\n",
              "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (attn): Attention(\n",
              "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
              "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
              "        )\n",
              "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): MLPBlock(\n",
              "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
              "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
              "          (act): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (neck): Sequential(\n",
              "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): LayerNorm2d()\n",
              "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (3): LayerNorm2d()\n",
              "    )\n",
              "  )\n",
              "  (prompt_encoder): PromptEncoder(\n",
              "    (pe_layer): PositionEmbeddingRandom()\n",
              "    (point_embeddings): ModuleList(\n",
              "      (0-3): 4 x Embedding(1, 256)\n",
              "    )\n",
              "    (not_a_point_embed): Embedding(1, 256)\n",
              "    (mask_downscaling): Sequential(\n",
              "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): LayerNorm2d()\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (4): LayerNorm2d()\n",
              "      (5): GELU(approximate='none')\n",
              "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "    (no_mask_embed): Embedding(1, 256)\n",
              "  )\n",
              "  (mask_decoder): MaskDecoder(\n",
              "    (transformer): TwoWayTransformer(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x TwoWayAttentionBlock(\n",
              "          (self_attn): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (cross_attn_token_to_image): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): MLPBlock(\n",
              "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
              "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
              "            (act): ReLU()\n",
              "          )\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (cross_attn_image_to_token): Attention(\n",
              "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (final_attn_token_to_image): Attention(\n",
              "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
              "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
              "      )\n",
              "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (iou_token): Embedding(1, 256)\n",
              "    (mask_tokens): Embedding(4, 256)\n",
              "    (output_upscaling): Sequential(\n",
              "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): LayerNorm2d()\n",
              "      (2): GELU(approximate='none')\n",
              "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (4): GELU(approximate='none')\n",
              "    )\n",
              "    (output_hypernetworks_mlps): ModuleList(\n",
              "      (0-3): 4 x MLP(\n",
              "        (layers): ModuleList(\n",
              "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
              "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (iou_prediction_head): MLP(\n",
              "      (layers): ModuleList(\n",
              "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
              "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable(['Modules', 'Parameters'])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        params = parameter.numel()\n",
        "        table.add_row([name, params])\n",
        "        total_params+=params\n",
        "    print(table)\n",
        "    print(f'Total Trainable Params: {total_params}')\n",
        "    return total_params"
      ],
      "metadata": {
        "id": "ZZqOIH_BDQpM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_parameters(sam)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpyJpB5CDZxN",
        "outputId": "29572c51-dd1f-4576-e8e4-86e49afc7298"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------------------------------------------+------------+\n",
            "|                                   Modules                                   | Parameters |\n",
            "+-----------------------------------------------------------------------------+------------+\n",
            "|                           image_encoder.pos_embed                           |  5242880   |\n",
            "|                    image_encoder.patch_embed.proj.weight                    |   983040   |\n",
            "|                     image_encoder.patch_embed.proj.bias                     |    1280    |\n",
            "|                     image_encoder.blocks.0.norm1.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.0.norm1.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.0.attn.rel_pos_h                    |    2160    |\n",
            "|                    image_encoder.blocks.0.attn.rel_pos_w                    |    2160    |\n",
            "|                    image_encoder.blocks.0.attn.qkv.weight                   |  4915200   |\n",
            "|                     image_encoder.blocks.0.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.0.attn.proj.weight                   |  1638400   |\n",
            "|                    image_encoder.blocks.0.attn.proj.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.0.norm2.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.0.norm2.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.0.mlp.lin1.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.0.mlp.lin1.bias                    |    5120    |\n",
            "|                    image_encoder.blocks.0.mlp.lin2.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.0.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.1.norm1.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.1.norm1.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.1.attn.rel_pos_h                    |    2160    |\n",
            "|                    image_encoder.blocks.1.attn.rel_pos_w                    |    2160    |\n",
            "|                    image_encoder.blocks.1.attn.qkv.weight                   |  4915200   |\n",
            "|                     image_encoder.blocks.1.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.1.attn.proj.weight                   |  1638400   |\n",
            "|                    image_encoder.blocks.1.attn.proj.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.1.norm2.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.1.norm2.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.1.mlp.lin1.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.1.mlp.lin1.bias                    |    5120    |\n",
            "|                    image_encoder.blocks.1.mlp.lin2.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.1.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.2.norm1.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.2.norm1.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.2.attn.rel_pos_h                    |    2160    |\n",
            "|                    image_encoder.blocks.2.attn.rel_pos_w                    |    2160    |\n",
            "|                    image_encoder.blocks.2.attn.qkv.weight                   |  4915200   |\n",
            "|                     image_encoder.blocks.2.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.2.attn.proj.weight                   |  1638400   |\n",
            "|                    image_encoder.blocks.2.attn.proj.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.2.norm2.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.2.norm2.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.2.mlp.lin1.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.2.mlp.lin1.bias                    |    5120    |\n",
            "|                    image_encoder.blocks.2.mlp.lin2.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.2.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.3.norm1.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.3.norm1.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.3.attn.rel_pos_h                    |    2160    |\n",
            "|                    image_encoder.blocks.3.attn.rel_pos_w                    |    2160    |\n",
            "|                    image_encoder.blocks.3.attn.qkv.weight                   |  4915200   |\n",
            "|                     image_encoder.blocks.3.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.3.attn.proj.weight                   |  1638400   |\n",
            "|                    image_encoder.blocks.3.attn.proj.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.3.norm2.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.3.norm2.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.3.mlp.lin1.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.3.mlp.lin1.bias                    |    5120    |\n",
            "|                    image_encoder.blocks.3.mlp.lin2.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.3.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.4.norm1.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.4.norm1.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.4.attn.rel_pos_h                    |    2160    |\n",
            "|                    image_encoder.blocks.4.attn.rel_pos_w                    |    2160    |\n",
            "|                    image_encoder.blocks.4.attn.qkv.weight                   |  4915200   |\n",
            "|                     image_encoder.blocks.4.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.4.attn.proj.weight                   |  1638400   |\n",
            "|                    image_encoder.blocks.4.attn.proj.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.4.norm2.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.4.norm2.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.4.mlp.lin1.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.4.mlp.lin1.bias                    |    5120    |\n",
            "|                    image_encoder.blocks.4.mlp.lin2.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.4.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.5.norm1.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.5.norm1.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.5.attn.rel_pos_h                    |    2160    |\n",
            "|                    image_encoder.blocks.5.attn.rel_pos_w                    |    2160    |\n",
            "|                    image_encoder.blocks.5.attn.qkv.weight                   |  4915200   |\n",
            "|                     image_encoder.blocks.5.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.5.attn.proj.weight                   |  1638400   |\n",
            "|                    image_encoder.blocks.5.attn.proj.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.5.norm2.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.5.norm2.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.5.mlp.lin1.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.5.mlp.lin1.bias                    |    5120    |\n",
            "|                    image_encoder.blocks.5.mlp.lin2.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.5.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.6.norm1.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.6.norm1.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.6.attn.rel_pos_h                    |    2160    |\n",
            "|                    image_encoder.blocks.6.attn.rel_pos_w                    |    2160    |\n",
            "|                    image_encoder.blocks.6.attn.qkv.weight                   |  4915200   |\n",
            "|                     image_encoder.blocks.6.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.6.attn.proj.weight                   |  1638400   |\n",
            "|                    image_encoder.blocks.6.attn.proj.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.6.norm2.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.6.norm2.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.6.mlp.lin1.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.6.mlp.lin1.bias                    |    5120    |\n",
            "|                    image_encoder.blocks.6.mlp.lin2.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.6.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.7.norm1.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.7.norm1.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.7.attn.rel_pos_h                    |   10160    |\n",
            "|                    image_encoder.blocks.7.attn.rel_pos_w                    |   10160    |\n",
            "|                    image_encoder.blocks.7.attn.qkv.weight                   |  4915200   |\n",
            "|                     image_encoder.blocks.7.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.7.attn.proj.weight                   |  1638400   |\n",
            "|                    image_encoder.blocks.7.attn.proj.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.7.norm2.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.7.norm2.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.7.mlp.lin1.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.7.mlp.lin1.bias                    |    5120    |\n",
            "|                    image_encoder.blocks.7.mlp.lin2.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.7.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.8.norm1.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.8.norm1.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.8.attn.rel_pos_h                    |    2160    |\n",
            "|                    image_encoder.blocks.8.attn.rel_pos_w                    |    2160    |\n",
            "|                    image_encoder.blocks.8.attn.qkv.weight                   |  4915200   |\n",
            "|                     image_encoder.blocks.8.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.8.attn.proj.weight                   |  1638400   |\n",
            "|                    image_encoder.blocks.8.attn.proj.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.8.norm2.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.8.norm2.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.8.mlp.lin1.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.8.mlp.lin1.bias                    |    5120    |\n",
            "|                    image_encoder.blocks.8.mlp.lin2.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.8.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.9.norm1.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.9.norm1.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.9.attn.rel_pos_h                    |    2160    |\n",
            "|                    image_encoder.blocks.9.attn.rel_pos_w                    |    2160    |\n",
            "|                    image_encoder.blocks.9.attn.qkv.weight                   |  4915200   |\n",
            "|                     image_encoder.blocks.9.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.9.attn.proj.weight                   |  1638400   |\n",
            "|                    image_encoder.blocks.9.attn.proj.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.9.norm2.weight                     |    1280    |\n",
            "|                      image_encoder.blocks.9.norm2.bias                      |    1280    |\n",
            "|                    image_encoder.blocks.9.mlp.lin1.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.9.mlp.lin1.bias                    |    5120    |\n",
            "|                    image_encoder.blocks.9.mlp.lin2.weight                   |  6553600   |\n",
            "|                     image_encoder.blocks.9.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.10.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.10.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.10.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.10.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.10.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.10.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.10.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.10.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.10.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.10.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.10.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.10.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.10.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.10.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.11.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.11.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.11.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.11.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.11.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.11.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.11.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.11.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.11.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.11.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.11.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.11.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.11.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.11.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.12.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.12.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.12.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.12.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.12.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.12.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.12.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.12.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.12.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.12.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.12.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.12.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.12.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.12.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.13.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.13.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.13.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.13.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.13.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.13.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.13.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.13.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.13.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.13.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.13.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.13.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.13.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.13.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.14.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.14.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.14.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.14.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.14.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.14.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.14.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.14.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.14.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.14.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.14.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.14.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.14.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.14.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.15.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.15.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.15.attn.rel_pos_h                   |   10160    |\n",
            "|                    image_encoder.blocks.15.attn.rel_pos_w                   |   10160    |\n",
            "|                   image_encoder.blocks.15.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.15.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.15.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.15.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.15.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.15.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.15.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.15.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.15.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.15.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.16.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.16.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.16.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.16.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.16.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.16.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.16.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.16.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.16.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.16.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.16.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.16.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.16.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.16.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.17.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.17.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.17.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.17.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.17.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.17.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.17.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.17.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.17.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.17.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.17.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.17.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.17.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.17.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.18.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.18.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.18.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.18.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.18.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.18.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.18.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.18.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.18.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.18.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.18.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.18.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.18.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.18.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.19.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.19.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.19.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.19.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.19.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.19.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.19.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.19.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.19.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.19.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.19.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.19.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.19.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.19.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.20.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.20.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.20.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.20.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.20.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.20.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.20.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.20.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.20.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.20.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.20.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.20.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.20.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.20.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.21.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.21.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.21.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.21.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.21.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.21.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.21.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.21.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.21.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.21.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.21.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.21.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.21.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.21.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.22.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.22.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.22.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.22.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.22.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.22.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.22.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.22.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.22.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.22.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.22.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.22.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.22.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.22.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.23.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.23.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.23.attn.rel_pos_h                   |   10160    |\n",
            "|                    image_encoder.blocks.23.attn.rel_pos_w                   |   10160    |\n",
            "|                   image_encoder.blocks.23.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.23.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.23.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.23.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.23.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.23.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.23.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.23.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.23.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.23.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.24.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.24.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.24.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.24.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.24.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.24.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.24.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.24.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.24.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.24.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.24.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.24.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.24.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.24.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.25.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.25.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.25.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.25.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.25.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.25.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.25.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.25.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.25.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.25.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.25.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.25.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.25.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.25.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.26.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.26.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.26.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.26.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.26.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.26.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.26.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.26.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.26.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.26.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.26.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.26.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.26.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.26.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.27.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.27.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.27.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.27.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.27.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.27.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.27.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.27.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.27.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.27.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.27.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.27.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.27.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.27.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.28.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.28.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.28.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.28.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.28.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.28.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.28.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.28.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.28.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.28.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.28.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.28.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.28.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.28.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.29.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.29.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.29.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.29.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.29.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.29.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.29.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.29.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.29.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.29.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.29.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.29.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.29.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.29.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.30.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.30.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.30.attn.rel_pos_h                   |    2160    |\n",
            "|                    image_encoder.blocks.30.attn.rel_pos_w                   |    2160    |\n",
            "|                   image_encoder.blocks.30.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.30.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.30.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.30.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.30.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.30.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.30.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.30.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.30.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.30.mlp.lin2.bias                    |    1280    |\n",
            "|                     image_encoder.blocks.31.norm1.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.31.norm1.bias                     |    1280    |\n",
            "|                    image_encoder.blocks.31.attn.rel_pos_h                   |   10160    |\n",
            "|                    image_encoder.blocks.31.attn.rel_pos_w                   |   10160    |\n",
            "|                   image_encoder.blocks.31.attn.qkv.weight                   |  4915200   |\n",
            "|                    image_encoder.blocks.31.attn.qkv.bias                    |    3840    |\n",
            "|                   image_encoder.blocks.31.attn.proj.weight                  |  1638400   |\n",
            "|                    image_encoder.blocks.31.attn.proj.bias                   |    1280    |\n",
            "|                     image_encoder.blocks.31.norm2.weight                    |    1280    |\n",
            "|                      image_encoder.blocks.31.norm2.bias                     |    1280    |\n",
            "|                   image_encoder.blocks.31.mlp.lin1.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.31.mlp.lin1.bias                    |    5120    |\n",
            "|                   image_encoder.blocks.31.mlp.lin2.weight                   |  6553600   |\n",
            "|                    image_encoder.blocks.31.mlp.lin2.bias                    |    1280    |\n",
            "|                         image_encoder.neck.0.weight                         |   327680   |\n",
            "|                         image_encoder.neck.1.weight                         |    256     |\n",
            "|                          image_encoder.neck.1.bias                          |    256     |\n",
            "|                         image_encoder.neck.2.weight                         |   589824   |\n",
            "|                         image_encoder.neck.3.weight                         |    256     |\n",
            "|                          image_encoder.neck.3.bias                          |    256     |\n",
            "|                   prompt_encoder.point_embeddings.0.weight                  |    256     |\n",
            "|                   prompt_encoder.point_embeddings.1.weight                  |    256     |\n",
            "|                   prompt_encoder.point_embeddings.2.weight                  |    256     |\n",
            "|                   prompt_encoder.point_embeddings.3.weight                  |    256     |\n",
            "|                   prompt_encoder.not_a_point_embed.weight                   |    256     |\n",
            "|                   prompt_encoder.mask_downscaling.0.weight                  |     16     |\n",
            "|                    prompt_encoder.mask_downscaling.0.bias                   |     4      |\n",
            "|                   prompt_encoder.mask_downscaling.1.weight                  |     4      |\n",
            "|                    prompt_encoder.mask_downscaling.1.bias                   |     4      |\n",
            "|                   prompt_encoder.mask_downscaling.3.weight                  |    256     |\n",
            "|                    prompt_encoder.mask_downscaling.3.bias                   |     16     |\n",
            "|                   prompt_encoder.mask_downscaling.4.weight                  |     16     |\n",
            "|                    prompt_encoder.mask_downscaling.4.bias                   |     16     |\n",
            "|                   prompt_encoder.mask_downscaling.6.weight                  |    4096    |\n",
            "|                    prompt_encoder.mask_downscaling.6.bias                   |    256     |\n",
            "|                     prompt_encoder.no_mask_embed.weight                     |    256     |\n",
            "|          mask_decoder.transformer.layers.0.self_attn.q_proj.weight          |   65536    |\n",
            "|           mask_decoder.transformer.layers.0.self_attn.q_proj.bias           |    256     |\n",
            "|          mask_decoder.transformer.layers.0.self_attn.k_proj.weight          |   65536    |\n",
            "|           mask_decoder.transformer.layers.0.self_attn.k_proj.bias           |    256     |\n",
            "|          mask_decoder.transformer.layers.0.self_attn.v_proj.weight          |   65536    |\n",
            "|           mask_decoder.transformer.layers.0.self_attn.v_proj.bias           |    256     |\n",
            "|         mask_decoder.transformer.layers.0.self_attn.out_proj.weight         |   65536    |\n",
            "|          mask_decoder.transformer.layers.0.self_attn.out_proj.bias          |    256     |\n",
            "|                mask_decoder.transformer.layers.0.norm1.weight               |    256     |\n",
            "|                 mask_decoder.transformer.layers.0.norm1.bias                |    256     |\n",
            "|  mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight  |   32768    |\n",
            "|   mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias   |    128     |\n",
            "|  mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight  |   32768    |\n",
            "|   mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias   |    128     |\n",
            "|  mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight  |   32768    |\n",
            "|   mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias   |    128     |\n",
            "| mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight |   32768    |\n",
            "|  mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias  |    256     |\n",
            "|                mask_decoder.transformer.layers.0.norm2.weight               |    256     |\n",
            "|                 mask_decoder.transformer.layers.0.norm2.bias                |    256     |\n",
            "|              mask_decoder.transformer.layers.0.mlp.lin1.weight              |   524288   |\n",
            "|               mask_decoder.transformer.layers.0.mlp.lin1.bias               |    2048    |\n",
            "|              mask_decoder.transformer.layers.0.mlp.lin2.weight              |   524288   |\n",
            "|               mask_decoder.transformer.layers.0.mlp.lin2.bias               |    256     |\n",
            "|                mask_decoder.transformer.layers.0.norm3.weight               |    256     |\n",
            "|                 mask_decoder.transformer.layers.0.norm3.bias                |    256     |\n",
            "|                mask_decoder.transformer.layers.0.norm4.weight               |    256     |\n",
            "|                 mask_decoder.transformer.layers.0.norm4.bias                |    256     |\n",
            "|  mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight  |   32768    |\n",
            "|   mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias   |    128     |\n",
            "|  mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight  |   32768    |\n",
            "|   mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias   |    128     |\n",
            "|  mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight  |   32768    |\n",
            "|   mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias   |    128     |\n",
            "| mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight |   32768    |\n",
            "|  mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias  |    256     |\n",
            "|          mask_decoder.transformer.layers.1.self_attn.q_proj.weight          |   65536    |\n",
            "|           mask_decoder.transformer.layers.1.self_attn.q_proj.bias           |    256     |\n",
            "|          mask_decoder.transformer.layers.1.self_attn.k_proj.weight          |   65536    |\n",
            "|           mask_decoder.transformer.layers.1.self_attn.k_proj.bias           |    256     |\n",
            "|          mask_decoder.transformer.layers.1.self_attn.v_proj.weight          |   65536    |\n",
            "|           mask_decoder.transformer.layers.1.self_attn.v_proj.bias           |    256     |\n",
            "|         mask_decoder.transformer.layers.1.self_attn.out_proj.weight         |   65536    |\n",
            "|          mask_decoder.transformer.layers.1.self_attn.out_proj.bias          |    256     |\n",
            "|                mask_decoder.transformer.layers.1.norm1.weight               |    256     |\n",
            "|                 mask_decoder.transformer.layers.1.norm1.bias                |    256     |\n",
            "|  mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight  |   32768    |\n",
            "|   mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias   |    128     |\n",
            "|  mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight  |   32768    |\n",
            "|   mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias   |    128     |\n",
            "|  mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight  |   32768    |\n",
            "|   mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias   |    128     |\n",
            "| mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight |   32768    |\n",
            "|  mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias  |    256     |\n",
            "|                mask_decoder.transformer.layers.1.norm2.weight               |    256     |\n",
            "|                 mask_decoder.transformer.layers.1.norm2.bias                |    256     |\n",
            "|              mask_decoder.transformer.layers.1.mlp.lin1.weight              |   524288   |\n",
            "|               mask_decoder.transformer.layers.1.mlp.lin1.bias               |    2048    |\n",
            "|              mask_decoder.transformer.layers.1.mlp.lin2.weight              |   524288   |\n",
            "|               mask_decoder.transformer.layers.1.mlp.lin2.bias               |    256     |\n",
            "|                mask_decoder.transformer.layers.1.norm3.weight               |    256     |\n",
            "|                 mask_decoder.transformer.layers.1.norm3.bias                |    256     |\n",
            "|                mask_decoder.transformer.layers.1.norm4.weight               |    256     |\n",
            "|                 mask_decoder.transformer.layers.1.norm4.bias                |    256     |\n",
            "|  mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight  |   32768    |\n",
            "|   mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias   |    128     |\n",
            "|  mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight  |   32768    |\n",
            "|   mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias   |    128     |\n",
            "|  mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight  |   32768    |\n",
            "|   mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias   |    128     |\n",
            "| mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight |   32768    |\n",
            "|  mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias  |    256     |\n",
            "|       mask_decoder.transformer.final_attn_token_to_image.q_proj.weight      |   32768    |\n",
            "|        mask_decoder.transformer.final_attn_token_to_image.q_proj.bias       |    128     |\n",
            "|       mask_decoder.transformer.final_attn_token_to_image.k_proj.weight      |   32768    |\n",
            "|        mask_decoder.transformer.final_attn_token_to_image.k_proj.bias       |    128     |\n",
            "|       mask_decoder.transformer.final_attn_token_to_image.v_proj.weight      |   32768    |\n",
            "|        mask_decoder.transformer.final_attn_token_to_image.v_proj.bias       |    128     |\n",
            "|      mask_decoder.transformer.final_attn_token_to_image.out_proj.weight     |   32768    |\n",
            "|       mask_decoder.transformer.final_attn_token_to_image.out_proj.bias      |    256     |\n",
            "|               mask_decoder.transformer.norm_final_attn.weight               |    256     |\n",
            "|                mask_decoder.transformer.norm_final_attn.bias                |    256     |\n",
            "|                        mask_decoder.iou_token.weight                        |    256     |\n",
            "|                       mask_decoder.mask_tokens.weight                       |    1024    |\n",
            "|                    mask_decoder.output_upscaling.0.weight                   |   65536    |\n",
            "|                     mask_decoder.output_upscaling.0.bias                    |     64     |\n",
            "|                    mask_decoder.output_upscaling.1.weight                   |     64     |\n",
            "|                     mask_decoder.output_upscaling.1.bias                    |     64     |\n",
            "|                    mask_decoder.output_upscaling.3.weight                   |    8192    |\n",
            "|                     mask_decoder.output_upscaling.3.bias                    |     32     |\n",
            "|           mask_decoder.output_hypernetworks_mlps.0.layers.0.weight          |   65536    |\n",
            "|            mask_decoder.output_hypernetworks_mlps.0.layers.0.bias           |    256     |\n",
            "|           mask_decoder.output_hypernetworks_mlps.0.layers.1.weight          |   65536    |\n",
            "|            mask_decoder.output_hypernetworks_mlps.0.layers.1.bias           |    256     |\n",
            "|           mask_decoder.output_hypernetworks_mlps.0.layers.2.weight          |    8192    |\n",
            "|            mask_decoder.output_hypernetworks_mlps.0.layers.2.bias           |     32     |\n",
            "|           mask_decoder.output_hypernetworks_mlps.1.layers.0.weight          |   65536    |\n",
            "|            mask_decoder.output_hypernetworks_mlps.1.layers.0.bias           |    256     |\n",
            "|           mask_decoder.output_hypernetworks_mlps.1.layers.1.weight          |   65536    |\n",
            "|            mask_decoder.output_hypernetworks_mlps.1.layers.1.bias           |    256     |\n",
            "|           mask_decoder.output_hypernetworks_mlps.1.layers.2.weight          |    8192    |\n",
            "|            mask_decoder.output_hypernetworks_mlps.1.layers.2.bias           |     32     |\n",
            "|           mask_decoder.output_hypernetworks_mlps.2.layers.0.weight          |   65536    |\n",
            "|            mask_decoder.output_hypernetworks_mlps.2.layers.0.bias           |    256     |\n",
            "|           mask_decoder.output_hypernetworks_mlps.2.layers.1.weight          |   65536    |\n",
            "|            mask_decoder.output_hypernetworks_mlps.2.layers.1.bias           |    256     |\n",
            "|           mask_decoder.output_hypernetworks_mlps.2.layers.2.weight          |    8192    |\n",
            "|            mask_decoder.output_hypernetworks_mlps.2.layers.2.bias           |     32     |\n",
            "|           mask_decoder.output_hypernetworks_mlps.3.layers.0.weight          |   65536    |\n",
            "|            mask_decoder.output_hypernetworks_mlps.3.layers.0.bias           |    256     |\n",
            "|           mask_decoder.output_hypernetworks_mlps.3.layers.1.weight          |   65536    |\n",
            "|            mask_decoder.output_hypernetworks_mlps.3.layers.1.bias           |    256     |\n",
            "|           mask_decoder.output_hypernetworks_mlps.3.layers.2.weight          |    8192    |\n",
            "|            mask_decoder.output_hypernetworks_mlps.3.layers.2.bias           |     32     |\n",
            "|               mask_decoder.iou_prediction_head.layers.0.weight              |   65536    |\n",
            "|                mask_decoder.iou_prediction_head.layers.0.bias               |    256     |\n",
            "|               mask_decoder.iou_prediction_head.layers.1.weight              |   65536    |\n",
            "|                mask_decoder.iou_prediction_head.layers.1.bias               |    256     |\n",
            "|               mask_decoder.iou_prediction_head.layers.2.weight              |    1024    |\n",
            "|                mask_decoder.iou_prediction_head.layers.2.bias               |     4      |\n",
            "+-----------------------------------------------------------------------------+------------+\n",
            "Total Trainable Params: 641090608\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "641090608"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while 1:pass"
      ],
      "metadata": {
        "id": "LJ5Fzl-5E29M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### load dataset"
      ],
      "metadata": {
        "id": "b7kE94547UaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset with masks\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir, mask_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(self.root_dir)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = os.path.join(self.root_dir, self.images[idx])\n",
        "        mask_name = os.path.join(self.mask_dir, self.images[idx])\n",
        "\n",
        "        image = Image.open(img_name)\n",
        "        mask = Image.open(mask_name)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = self.transform(mask)\n",
        "\n",
        "        return image, mask\n"
      ],
      "metadata": {
        "id": "64DMzAev0WC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sam.named_parameters()"
      ],
      "metadata": {
        "id": "GuJcXNoAngmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from prettytable import PrettyTable\n",
        "def count_trainablelayers(model):\n",
        "    trainable = 0\n",
        "    table = PrettyTable(['Modules', 'Gradient'])\n",
        "\n",
        "    for name, parameter in model.named_parameters():\n",
        "        table.add_row([name, parameter.requires_grad])\n",
        "        trainable +=1\n",
        "    print(table)\n",
        "\n",
        "    return trainable\n",
        "\n",
        "count_trainablelayers(sam)"
      ],
      "metadata": {
        "id": "d8noMtF60d0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_layer(model, layernameroot):\n",
        "    trainable = 0\n",
        "    table = PrettyTable(['Modules', 'Gradient'])\n",
        "\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not name.startswith(layernameroot):\n",
        "          #print(\"here\", name)\n",
        "          parameter.requires_grad = False\n",
        "        table.add_row([name, parameter.requires_grad])\n",
        "        if parameter.requires_grad:\n",
        "          trainable +=1\n",
        "    print(table)\n",
        "\n",
        "    return trainable\n",
        "\n",
        "ntrainable = freeze_layer(sam, 'mask_decoder.iou_prediction_head')\n",
        "torch.save(model.state_dict(), f\"samLE_funfrozen{ntrainable}.pth\")"
      ],
      "metadata": {
        "id": "4BbGP0jcn5L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HMRqyb_LonJO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}